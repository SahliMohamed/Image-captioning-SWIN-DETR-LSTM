{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a395b46f-1095-4a07-adc9-73d49fe78991",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import string\n",
    "import csv\n",
    "import pickle\n",
    "import timm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.optim import Adam\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from transformers import DetrForObjectDetection, DetrFeatureExtractor\n",
    "\n",
    "data_dir = os.path.join('dataset')\n",
    "working_dir = os.path.join('working')\n",
    "images_dir = os.path.join(data_dir,'Images')\n",
    "captions_dir = os.path.join(data_dir,'captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21899a99-dfd8-4c91-a52f-63218bd17d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA GeForce GTX 1660 SUPER\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "# *********************\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b6e3d5-ec75-45a5-8a5a-d7f6970e6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "pretrained_embeddings_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, \n",
    "binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2dc589-c1f7-4ff5-94a2-f7008abb8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# # Example text data\n",
    "# documents = [\"This is the first sentence.\", \"Here is another one.\", \"And a third sentence.\"]\n",
    "\n",
    "# # Preprocess text\n",
    "# processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9316cbbe-44bf-4310-9985-ed082178a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eded2ad0-2035-4731-a85e-4bee171d92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for flickr8k\n",
    "def load_captions(filepath):\n",
    "    captions = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.reader(file) \n",
    "        for row in reader:\n",
    "            if len(row) != 2:\n",
    "                print(f\"Skipping malformed line: {row[:50]}...\")  \n",
    "                continue\n",
    "            image_id, caption = row\n",
    "            image_id = image_id.split('.')[0]  \n",
    "            if image_id not in captions:\n",
    "                captions[image_id] = []\n",
    "            captions[image_id].append(caption)\n",
    "    \n",
    "    return captions\n",
    "captions = load_captions(captions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787e6f8f-1059-476e-a1d2-f7430fa2dc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # for flickr30k\n",
    "# def load_captions(filepath):\n",
    "#     captions = {}\n",
    "#     # Adding encoding parameter to handle potential encoding issues\n",
    "#     with open(filepath, 'r', encoding='utf-8', errors='replace') as file:\n",
    "#         reader = csv.reader(file) \n",
    "#         for row in reader:\n",
    "#             if len(row) != 2:\n",
    "#                 print(f\"Skipping malformed line: {row[:50]}...\")  \n",
    "#                 continue\n",
    "#             image_id, caption = row\n",
    "#             image_id = image_id.split('.')[0]  \n",
    "#             if image_id not in captions:\n",
    "#                 captions[image_id] = []\n",
    "#             captions[image_id].append(caption)\n",
    "    \n",
    "#     return captions\n",
    "\n",
    "# captions = load_captions(captions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7ff5c4e-9e37-4e3a-8ab1-f26711510ba3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(caption):\n",
    "    tokens = caption.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return tokens\n",
    "    \n",
    "# Collect all captions\n",
    "all_captions = []\n",
    "for cap_list in captions.values():\n",
    "    all_captions.extend(cap_list)\n",
    "    \n",
    "# Count word frequencies\n",
    "word_counts = Counter()\n",
    "for caption in all_captions:\n",
    "    word_counts.update(clean_and_tokenize(caption))\n",
    "\n",
    "# Create a vocabulary with words that exist in both Word2Vec and your dataset\n",
    "vocab = [word for word, count in word_counts.items() if count >= 2]\n",
    "\n",
    "# Map words to indices for the special tokens\n",
    "word_to_ix = {word: ix for ix, word in enumerate(vocab, start=4)}  # start=4 to leave 0 for <PAD>, 1 for <START>, 2 for <END>, 3 for <UNK>\n",
    "word_to_ix['<PAD>'] = 0\n",
    "word_to_ix['<START>'] = 1\n",
    "word_to_ix['<END>'] = 2\n",
    "word_to_ix['<UNK>'] = 3\n",
    "\n",
    "# Reverse lookup for decoding\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "# Update vocab size\n",
    "vocab_size = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58251b2-9575-4997-81f4-f1e15ebeec61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_file_path = 'vocab.json'\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(word_to_ix, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e039743-790c-4625-bcde-61a78447b1ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_caption(caption, word_to_ix, max_length):\n",
    "    tokens = clean_and_tokenize(caption)\n",
    "    tokens = ['<START>'] + tokens + ['<END>']\n",
    "    caption_ids = [word_to_ix.get(token, word_to_ix['<UNK>']) for token in tokens]\n",
    "    if len(caption_ids) < max_length:\n",
    "        caption_ids += [word_to_ix['<PAD>']] * (max_length - len(caption_ids))\n",
    "    else:\n",
    "        caption_ids = caption_ids[:max_length]\n",
    "    return np.array(caption_ids)\n",
    "\n",
    "max_length = max(len(clean_and_tokenize(caption)) + 2 for caption in all_captions)  # +2 for <START> and <END>\n",
    "encoded_captions = {img_id: [encode_caption(caption, word_to_ix, max_length) for caption in cap_list]\n",
    "                    for img_id, cap_list in captions.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b461bbef-dab1-410f-8cde-5827cc458c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of image IDs (filenames without extension)\n",
    "image_ids = [img_name.split('.')[0] for img_name in os.listdir(images_dir) if img_name.endswith('.jpg')]\n",
    "\n",
    "# Set up transforms for training and validation/test\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Define the transform used during feature extraction (should be fixed)\n",
    "feature_extraction_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cc0978d-b76b-4fcc-9655-ba170a3ea9ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\anaconda3\\Lib\\site-packages\\transformers\\models\\detr\\feature_extraction_detr.py:38: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=92, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
    "# Load pretrained DETR model and feature extractor\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee737071-ecab-4a14-91b5-0676d65ac504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(image_path):\n",
    "    # Load image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((224, 224))  # Adjust the size as needed\n",
    "    \n",
    "    # Prepare image for DETR (Resize, normalize, etc.)\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Extract pixel values and create pixel mask\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get outputs from the model (including decoder)\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    # Extract the decoder hidden states (object queries from last decoder layer)\n",
    "    object_queries = outputs.last_hidden_state  # Last decoder layer output\n",
    "    \n",
    "    # Flatten the object queries to get a 2D tensor\n",
    "    flattened_object_queries = object_queries.view(object_queries.size(0), -1)  # Flatten to [batch_size, num_queries * hidden_dim]\n",
    "\n",
    "    # Move the pooled object queries to the CPU and return them\n",
    "    return flattened_object_queries.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c889cebc-24c0-410e-b442-74c55ba1ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract features for all images and save them in a dictionary\n",
    "# features_dict = {}\n",
    "# for image_id in tqdm(image_ids):\n",
    "#     image_path = os.path.join(images_dir, image_id + '.jpg')\n",
    "#     if os.path.exists(image_path):\n",
    "        \n",
    "#         # Extract features using DETR\n",
    "#         features = extract_features(image_path)\n",
    "#         features_dict[image_id] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0848b565-2a86-4244-9f13-daad0ac942c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save features to a .pkl file\n",
    "# with open(\"flickr8k_detr_features.pkl\", 'wb') as f:\n",
    "#         pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ba1510-8d9e-4b83-affc-07d3e572a063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from a .pkl file\n",
    "with open('flickr8k_detr_features.pkl', 'rb') as f:  # 'rb' for read-binary\n",
    "    features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82ef551a-52c2-40d2-90c4-cb6d31747af3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save 30k features to a .pkl file\n",
    "# with open('flickr30k_detr_features.pkl', 'wb') as f:  # 'wb' for write-binary\n",
    "#     pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05940b97-d9d8-496c-83d9-18b2592d07c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load 30kfeatures from a .pkl file\n",
    "# with open('flickr30k_detr_features.pkl', 'rb') as f:  # 'rb' for read-binary\n",
    "#     features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc41097-1c3f-407f-801c-df8389e64ff7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, features_dict , captions, encoded_captions, image_ids, transform):\n",
    "        self.features_dict  = features_dict \n",
    "        self.captions = captions\n",
    "        self.encoded_captions = encoded_captions\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        if image_id not in self.features_dict:\n",
    "            # Handle missing features, e.g., skip or use a placeholder\n",
    "            print(f\"Missing features for image ID: {image_id}\")\n",
    "            return None  # Or handle it appropriately\n",
    "\n",
    "        features = self.features_dict[image_id].clone().detach().float()\n",
    "        # Randomly choose one of the captions for this image\n",
    "        # captions_for_image = self.encoded_captions[image_id]\n",
    "        # caption_idx = np.random.randint(0, len(captions_for_image))  # Choose a random caption\n",
    "        \n",
    "        captions_for_image = np.array(self.encoded_captions[image_id])# caption = torch.tensor(captions_for_image[caption_idx], dtype=torch.long)\n",
    "        caption = torch.tensor(captions_for_image, dtype=torch.long)\n",
    "        \n",
    "        return features, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdc17753-380e-49e4-9573-fc4f2441c6f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Filter out None values\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if len(batch) == 0:\n",
    "        return torch.tensor([]).to(device), torch.tensor([]).to(device)  # Return empty tensors if the batch is empty\n",
    "        \n",
    "    # Unpack the batch into features and captions\n",
    "    features, captions = zip(*batch)\n",
    "\n",
    "    # Determine the maximum feature size in the batch\n",
    "    max_feature_size = max(feature.size(1) for feature in features)\n",
    "\n",
    "    # Pad feature tensors to the maximum sequence length\n",
    "    features = torch.stack([\n",
    "        torch.cat((feature.to(device), \n",
    "                   torch.zeros((feature.size(0), max_feature_size - feature.size(1), feature.size(2)), device=device)), dim=1)\n",
    "        for feature in features\n",
    "    ])\n",
    "    \n",
    "    # Stack the captions as they seem to have consistent shape\n",
    "    captions = torch.stack(captions).to(device)\n",
    "    \n",
    "    return features, captions\n",
    "\n",
    "    \n",
    "print(type(captions))  # This should print <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b59ec87e-45e2-4dcf-ad9e-a7c032348497",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random # Shuffle and split data\n",
    "image_ids = list(captions.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "\n",
    "# Calculate indices for splits\n",
    "total_images = len(image_ids)\n",
    "train_end = int(0.7 * total_images)\n",
    "val_end = int(0.9 * total_images)\n",
    "\n",
    "train_ids = image_ids[:train_end]\n",
    "val_ids = image_ids[train_end:val_end]\n",
    "test_ids = image_ids[val_end:]\n",
    "\n",
    "train_dataset = Flickr8kDataset(features_dict, captions, encoded_captions, train_ids, train_transform)\n",
    "val_dataset = Flickr8kDataset(features_dict, captions, encoded_captions, val_ids, test_transform)\n",
    "test_dataset = Flickr8kDataset(features_dict, captions, encoded_captions, test_ids, test_transform)\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcfc9f81-3c94-4d4a-8e65-e12718359db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Size: 25600\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"dataset/Images/133905560_9d012b47f3.jpg\"  # Replace with a valid image path\n",
    "sample_features = extract_features(sample_image_path)\n",
    "feature_size = sample_features.size(1)\n",
    "print(\"Feature Size:\", feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b09c47d-2813-4777-aa7b-58e0cde46e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, vocab_size, embed_size, dropout):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)  # Dropout after LSTM\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.feature_fc = nn.Linear(feature_size, embed_size)  # Adapt DETR output to LSTM\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Project features to embedding size\n",
    "        batch_size = features.size(0)\n",
    "        features = features.view(batch_size, -1)  # Flatten the feature tensor\n",
    "        features = self.feature_fc(features).unsqueeze(1)  # [batch_size, 1, embed_size]\n",
    "        \n",
    "        # Prepare LSTM inputs\n",
    "        embeddings = self.embedding(captions)\n",
    "        embeddings = self.dropout(embeddings)  # Dropout after embedding layer\n",
    "        inputs = torch.cat((features, embeddings[:, :-1, :]), dim=1)  # Concatenate features with captions\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        hiddens = self.lstm_dropout(hiddens)  # Dropout after LSTM hidden states\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "# Model parameters\n",
    "feature_size = 6400  # or 25600 depending on your actual feature size\n",
    "hidden_size = 1024\n",
    "embed_size = 256\n",
    "dropout = 0.5\n",
    "vocab_size = len(word_to_ix)  # Ensure you have defined word_to_ix\n",
    "\n",
    "# Instantiate the model\n",
    "captioning_model = ImageCaptioningModel(feature_size, hidden_size, vocab_size, embed_size, dropout)\n",
    "captioning_model = captioning_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c68906ec-289a-4b6d-84c5-7586e9d4c723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2c73c28-deb9-4d00-9e85-025ed63d522c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/40]:   3%|▎         | 229/9120 [00:27<09:07, 16.25it/s, Step=0, Loss=3.48]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Validation Loss: 3.4370822158514285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/40]:   5%|▌         | 457/9120 [00:55<08:59, 16.06it/s, Step=0, Loss=3.24]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Validation Loss: 3.2260530742944455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/40]:   8%|▊         | 685/9120 [01:23<08:48, 15.96it/s, Step=0, Loss=3.12]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40], Validation Loss: 3.1042121204675412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/40]:  10%|█         | 913/9120 [01:51<08:42, 15.70it/s, Step=0, Loss=2.92]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40], Validation Loss: 3.0494663107628917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/40]:  13%|█▎        | 1141/9120 [02:20<08:28, 15.70it/s, Step=0, Loss=3.04]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40], Validation Loss: 3.0187244929519355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/40]:  15%|█▌        | 1369/9120 [02:49<08:18, 15.54it/s, Step=0, Loss=2.86]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40], Validation Loss: 2.982947630040786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/40]:  18%|█▊        | 1597/9120 [03:17<08:00, 15.66it/s, Step=0, Loss=2.93]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40], Validation Loss: 2.959239590401743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/40]:  20%|██        | 1825/9120 [03:46<07:48, 15.56it/s, Step=0, Loss=2.76]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40], Validation Loss: 2.937050637076883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/40]:  23%|██▎       | 2053/9120 [04:15<07:34, 15.54it/s, Step=0, Loss=3.02]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40], Validation Loss: 2.9347393559474573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/40]:  25%|██▌       | 2281/9120 [04:44<07:21, 15.50it/s, Step=0, Loss=2.79]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40], Validation Loss: 2.914818291570626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/40]:  28%|██▊       | 2509/9120 [05:12<07:13, 15.24it/s, Step=0, Loss=2.49]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40], Validation Loss: 2.8947014013926187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/40]:  30%|███       | 2737/9120 [05:41<06:56, 15.33it/s, Step=0, Loss=2.82]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40], Validation Loss: 2.9000027039471794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/40]:  33%|███▎      | 2965/9120 [06:10<06:44, 15.22it/s, Step=0, Loss=2.87]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40], Validation Loss: 2.8947843289842794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/40]:  35%|███▌      | 3193/9120 [06:39<06:29, 15.22it/s, Step=0, Loss=2.8]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40], Validation Loss: 2.8682611222360648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/40]:  38%|███▊      | 3421/9120 [07:09<06:14, 15.22it/s, Step=0, Loss=2.83]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40], Validation Loss: 2.8764285667269838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/40]:  40%|████      | 3649/9120 [07:38<06:02, 15.11it/s, Step=0, Loss=2.82]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40], Validation Loss: 2.870807778601553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/40]:  43%|████▎     | 3877/9120 [08:07<05:46, 15.13it/s, Step=0, Loss=2.76]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40], Validation Loss: 2.8754104071972417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/40]:  45%|████▌     | 4104/9120 [08:36<10:31,  7.94it/s, Validation Loss=2.87]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40], Validation Loss: 2.8687110601686965\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_ix['<PAD>'])\n",
    "optimizer = Adam(captioning_model.parameters(), lr=0.002, weight_decay=0.0001)\n",
    "num_epochs = 40\n",
    "early_stopping = EarlyStopping(patience=4, delta=0.01)  # Adjust patience and delta as needed\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# Create a tqdm instance to show global progress for training and validation\n",
    "total_steps = num_epochs * len(train_loader) + num_epochs * len(val_loader)\n",
    "with tqdm(total=total_steps, desc='Training Progress') as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        captioning_model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i, (features, captions) in enumerate(train_loader):\n",
    "            if features.shape[0] == 0:  # Skip if batch is empty\n",
    "                continue\n",
    "                \n",
    "            # Move inputs and targets to the selected device (GPU or CPU)\n",
    "            features = features.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_size = features.size(0)  # Batch size (e.g., 32)\n",
    "            num_captions = captions.size(1)  # Number of captions (e.g., 5)\n",
    "            sequence_length = features.size(2)  # Sequence length (25)\n",
    "            feature_size = features.size(3)  # Feature embedding size (256)\n",
    "\n",
    "            # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "            features = features.squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "\n",
    "            # Now expand the features to repeat them across the captions dimension\n",
    "            features = features.unsqueeze(1).expand(-1, num_captions, -1, -1)  # [batch_size, num_captions, sequence_length, feature_size]\n",
    "\n",
    "            # Flatten features for input into the model\n",
    "            features = features.contiguous().view(batch_size * num_captions, sequence_length, feature_size)\n",
    "            # Reshape captions as well\n",
    "            captions = captions.view(batch_size * num_captions, -1)  # Make sure to handle caption shape accordingly\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = captioning_model(features, captions)\n",
    "            loss = criterion(outputs[:, :captions.size(1), :].view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            pbar.set_postfix({'Step': i, 'Loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_loss_values.append(epoch_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        captioning_model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, captions in val_loader:\n",
    "                if features.shape[0] == 0:  # Skip empty batches\n",
    "                    continue\n",
    "                # Move inputs and targets to the selected device (GPU or CPU)\n",
    "                features = features.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                batch_size = features.size(0)  # Batch size (e.g., 32)\n",
    "                num_captions = captions.size(1)  # Number of captions (e.g., 5)\n",
    "                sequence_length = features.size(2)  # Sequence length (25)\n",
    "                feature_size = features.size(3)  # Feature embedding size (256)\n",
    "\n",
    "                # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "                features = features.squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "    \n",
    "                # Now expand the features to repeat them across the captions dimension\n",
    "                features = features.unsqueeze(1).expand(-1, num_captions, -1, -1)  # [batch_size, num_captions, sequence_length, feature_size]\n",
    "    \n",
    "                # Flatten features for input into the model\n",
    "                features = features.contiguous().view(batch_size * num_captions, sequence_length, feature_size)\n",
    "                # Reshape captions as well\n",
    "                captions = captions.view(batch_size * num_captions, -1)  # Make sure to handle caption shape accordingly\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = captioning_model(features, captions)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "                epoch_val_loss += loss.item()\n",
    "                pbar.update(1)  # Update progress bar for validation batches\n",
    "\n",
    "        epoch_val_loss /= len(val_loader)\n",
    "        val_loss_values.append(epoch_val_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {epoch_val_loss}')\n",
    "\n",
    "        # Update tqdm progress bar to reflect the completion of an epoch\n",
    "        pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        pbar.set_postfix({'Validation Loss': epoch_val_loss})\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(epoch_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7411abb0-1b3e-4a58-b438-101e9679ff7d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to captioning_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'captioning_model.pth'\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(captioning_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed4fac2-d20d-4d35-8c5a-db734c89339e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from captioning_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\AppData\\Local\\Temp\\ipykernel_5900\\3224590603.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  captioning_model.load_state_dict(torch.load(model_save_path))\n"
     ]
    }
   ],
   "source": [
    "# Define the path where the model is saved\n",
    "model_save_path = 'captioning_model.pth'\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "captioning_model.load_state_dict(torch.load(model_save_path))\n",
    "# Set the model to evaluation mode\n",
    "captioning_model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c45dd76-776c-427c-99ad-337740e89c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save loss values to a file\n",
    "with open('train_loss_values.json', 'w') as f:\n",
    "    json.dump(train_loss_values, f)\n",
    "# Save loss values to a file\n",
    "with open('val_loss_values.json', 'w') as f:\n",
    "    json.dump(val_loss_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75875a4d-a458-4c74-8a4b-7e30617fe81c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if not data:\n",
    "            print(f\"Warning: {filename} is empty.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {filename}.\")\n",
    "        return None\n",
    "\n",
    "# Load the loss values\n",
    "train_loss_values = load_json('train_loss_values.json')\n",
    "val_loss_values = load_json('val_loss_values.json')\n",
    "\n",
    "# show thier values\n",
    "train_loss_values ,val_loss_values\n",
    "\n",
    "# Simplified plot without specifying figsize\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.plot(val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81136913-e128-4feb-9683-25e339ad6f6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8863517229373636\n"
     ]
    }
   ],
   "source": [
    "# Testing phase\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for features, captions in test_loader:\n",
    "        if features.shape[0] == 0:  # Skip empty batches\n",
    "            continue\n",
    "        # Move inputs and targets to the selected device (GPU or CPU)\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "                \n",
    "        batch_size = features.size(0)  # Batch size (e.g., 32)\n",
    "        num_captions = captions.size(1)  # Number of captions (e.g., 5)\n",
    "        sequence_length = features.size(2)  # Sequence length (25)\n",
    "        feature_size = features.size(3)  # Feature embedding size (256)\n",
    "\n",
    "        # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "        features = features.squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "\n",
    "        # Now expand the features to repeat them across the captions dimension\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1, -1)  # [batch_size, num_captions, sequence_length, feature_size]\n",
    "\n",
    "        # Flatten features for input into the model\n",
    "        features = features.contiguous().view(batch_size * num_captions, sequence_length, feature_size)\n",
    "        # Reshape captions as well\n",
    "        captions = captions.view(batch_size * num_captions, -1)  # Make sure to handle caption shape accordingly\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Compute average test loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9652b12d-a02b-4ec8-92b9-886058a41389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the image to match the expected input for the DETR model\n",
    "def preprocess_image(image_path):\n",
    "     # Instead of checking for string, print and check what's being passed\n",
    "    print(f\"Preprocessing image, path: {image_path}, type: {type(image_path)}\")  \n",
    "    if not isinstance(image_path, str):\n",
    "        raise TypeError(\"image_path should be a string.\")\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = feature_extraction_transform(image)  # Apply your feature extraction transformation\n",
    "    image = image.unsqueeze(0)  # Add batch dimension (1, 3, H, W)\n",
    "    return image\n",
    "    \n",
    "# Extract image features for the captioning model\n",
    "def extract_image_features(image_path, model, device):\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        # Pass through the DETR model (it expects `pixel_values`)\n",
    "        features = model(pixel_values=image_tensor).last_hidden_state  # Extract features\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        features = features[:, :6400] \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb495eeb-0051-4e02-af54-40fdc9e99f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_repetitive_words(caption):\n",
    "    \"\"\"\n",
    "    Removes consecutive repetitive words from the caption.\n",
    "    \"\"\"\n",
    "    words = caption.split()\n",
    "    filtered_words = [words[0]]  # Initialize with the first word\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] != words[i - 1]:\n",
    "            filtered_words.append(words[i])\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def generate_caption(captioning_model, model, image_path, word_to_ix, ix_to_word, max_length,feature_size=feature_size, beam_size=5):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using the trained captioning model and DETR Transformer for feature extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Image Path: {image_path}\")  # Debugging step\n",
    "    if not isinstance(image_path, str):\n",
    "        raise TypeError(\"image_path should be a valid string representing the file path.\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Extract features from the image using the DETR model\n",
    "    with torch.no_grad():\n",
    "         # Preprocess the image and extract features\n",
    "        features = extract_image_features(image_path, model, device)  # Shape is [1, 6400] now\n",
    "        features = features.view(1, 6400)  # Ensure correct feature size\n",
    "        print(f\"Extracted feature shape: {features.shape}\")\n",
    "\n",
    "    # Initialize beam search with <START> token and a score of 0\n",
    "    beam = [([word_to_ix['<START>']], 0)]  # (sequence, score)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        new_beam = []\n",
    "        \n",
    "        for seq, score in beam:\n",
    "            caption_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)  # Convert sequence to tensor\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = captioning_model(features, caption_tensor)  # Forward pass through captioning model\n",
    "            \n",
    "            # Get top beam_size predictions for the next word\n",
    "            top_k_scores, top_k_ids = torch.topk(outputs[0, -1], beam_size)\n",
    "            \n",
    "            # Iterate over top beam_size predictions\n",
    "            for i in range(beam_size):\n",
    "                new_seq = seq + [top_k_ids[i].item()]  # Append new word to sequence\n",
    "                new_score = score + top_k_scores[i].item()  # Update score with log probability\n",
    "                new_beam.append((new_seq, new_score))\n",
    "\n",
    "        # Keep only the top beam_size sequences\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "        # Stop if any sequence has generated the <END> token\n",
    "        if any(seq[-1] == word_to_ix['<END>'] for seq, _ in beam):\n",
    "            break\n",
    "\n",
    "    # Choose the best sequence (the one with the highest score)\n",
    "    best_seq = max(beam, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Skip the <START> and <END> tokens and convert the indices back to words\n",
    "    caption = ' '.join([ix_to_word[ix] for ix in best_seq if ix not in [word_to_ix['<START>'], word_to_ix['<END>']]])\n",
    "\n",
    "    # Remove repetitive words from the caption\n",
    "    caption = remove_repetitive_words(caption)\n",
    "    \n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966362a-c137-401c-9315-c44622a970b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "# Path to the image you want to caption\n",
    "image_path = 'dataset/Images/207275121_ee4dfa0bf2.jpg'\n",
    "# Generate caption\n",
    "feature_size = 6400\n",
    "max_length = 30  # Maximum caption length\n",
    "\n",
    "# try:\n",
    "# Ensure synchronization before running the caption generation\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Generate caption using preloaded models\n",
    "caption = generate_caption(captioning_model, model, image_path, word_to_ix, ix_to_word, max_length, feature_size, beam_size=5)\n",
    "\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image to numpy array\n",
    "image_array = np.array(image)\n",
    "\n",
    "# Create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the image and add the caption\n",
    "ax.imshow(image_array)\n",
    "ax.axis('off')  # Hide axes\n",
    "ax.set_title(caption)\n",
    "\n",
    "# Save the image with caption before showing it\n",
    "fig.savefig('image_with_caption.png', bbox_inches='tight', pad_inches=0)  # Save the plot as a PNG file\n",
    "\n",
    "# Display the image and the caption\n",
    "plt.show()  # Ensure the image is displayed with the caption\n",
    "\n",
    "# Free resources by clearing the figure\n",
    "plt.close(fig)  # Close the current figure after saving the image\n",
    "\n",
    "# Manually clear GPU cache and force garbage collection\n",
    "torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "gc.collect()  # Force garbage collection to free RAM\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during caption generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e18673-f54e-4639-9e6c-837740caa592",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "082748cf-ebad-44e9-9538-34c454aa44eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:01<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8863517229373636\n",
      "BLEU-1 Score: 0.36587084078780974\n",
      "BLEU-2 Score: 0.22793517375361835\n",
      "BLEU-3 Score: 0.16567726853269638\n",
      "BLEU-4 Score: 0.1139180228254251\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import fractions\n",
    "\n",
    "# Custom fraction class to avoid the unexpected keyword argument '_normalize' issue\n",
    "class CustomFraction(fractions.Fraction):\n",
    "    def __new__(cls, numerator=0, denominator=None, _normalize=True):\n",
    "        return super().__new__(cls, numerator, denominator)\n",
    "\n",
    "# Override the modified_precision function in nltk\n",
    "import nltk.translate.bleu_score as bleu_score\n",
    "bleu_score.Fraction = CustomFraction\n",
    "\n",
    "# Testing phase with BLEU score calculation and tqdm progress bar\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "bleu_scores = []\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "# Lists to collect actual and predicted captions for BLEU score calculation\n",
    "actual_captions = []\n",
    "predicted_captions_list = []\n",
    "\n",
    "# Use tqdm to track the progress of the testing phase\n",
    "with torch.no_grad():\n",
    "    for features, captions in tqdm(test_loader, desc=\"Testing\"):\n",
    "        if features.shape[0] == 0:\n",
    "            continue\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "        batch_size = features.size(0)  # Batch size (e.g., 32)\n",
    "        num_captions = captions.size(1)  # Number of captions (e.g., 5)\n",
    "        sequence_length = features.size(2)  # Sequence length (25)\n",
    "        feature_size = features.size(3)  # Feature embedding size (256)\n",
    "\n",
    "        # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "        features = features.squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "\n",
    "        # Now expand the features to repeat them across the captions dimension\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1, -1)  # [batch_size, num_captions, sequence_length, feature_size]\n",
    "\n",
    "        # Flatten features for input into the model\n",
    "        features = features.contiguous().view(batch_size * num_captions, sequence_length, feature_size)\n",
    "        # Reshape captions as well\n",
    "        captions = captions.view(batch_size * num_captions, -1)  # Make sure to handle caption shape accordingly\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Decode the outputs and collect captions\n",
    "        predicted_captions = outputs.argmax(2).cpu().numpy()\n",
    "        for pred, actual in zip(predicted_captions, captions.cpu().numpy()):\n",
    "            pred_caption = [ix_to_word[ix] for ix in pred if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_caption = [ix_to_word[ix] for ix in actual if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_captions.append(actual_caption)\n",
    "            predicted_captions_list.append(pred_caption)\n",
    "\n",
    "# Calculate BLEU scores for all captions\n",
    "bleu1 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n",
    "bleu2 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
    "bleu3 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n",
    "bleu4 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
    "\n",
    "# Calculate average loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'BLEU-1 Score: {bleu1}')\n",
    "print(f'BLEU-2 Score: {bleu2}')\n",
    "print(f'BLEU-3 Score: {bleu3}')\n",
    "print(f'BLEU-4 Score: {bleu4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e49e7c22-a0b7-4db4-944c-bc9f2aeafe9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:01<00:00, 13.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.8863517229373636\n",
      "Average BLEU Score: 0.16445025920032783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import fractions\n",
    "\n",
    "# Custom fraction class to avoid the unexpected keyword argument '_normalize' issue\n",
    "class CustomFraction(fractions.Fraction):\n",
    "    def __new__(cls, numerator=0, denominator=None, _normalize=True):\n",
    "        return super().__new__(cls, numerator, denominator)\n",
    "\n",
    "# Override the modified_precision function in nltk\n",
    "import nltk.translate.bleu_score as bleu_score\n",
    "bleu_score.Fraction = CustomFraction\n",
    "\n",
    "# Testing phase with BLEU score calculation and tqdm progress bar\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "bleu_scores = []\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "# Use tqdm to track the progress of the testing phase\n",
    "with torch.no_grad():\n",
    "    for features, captions in tqdm(test_loader, desc=\"Testing\"):\n",
    "        if features.shape[0] == 0:\n",
    "            continue\n",
    "        # Move inputs and targets to the selected device (GPU or CPU)\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "                \n",
    "        batch_size = features.size(0)  # Batch size (e.g., 32)\n",
    "        num_captions = captions.size(1)  # Number of captions (e.g., 5)\n",
    "        sequence_length = features.size(2)  # Sequence length (25)\n",
    "        feature_size = features.size(3)  # Feature embedding size (256)\n",
    "\n",
    "        # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "        features = features.squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "\n",
    "        # Now expand the features to repeat them across the captions dimension\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1, -1)  # [batch_size, num_captions, sequence_length, feature_size]\n",
    "\n",
    "        # Flatten features for input into the model\n",
    "        features = features.contiguous().view(batch_size * num_captions, sequence_length, feature_size)\n",
    "        # Reshape captions as well\n",
    "        captions = captions.view(batch_size * num_captions, -1)  # Make sure to handle caption shape accordingly\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Decode the outputs and calculate BLEU scores\n",
    "        predicted_captions = outputs.argmax(2).cpu().numpy()\n",
    "        for pred, actual in zip(predicted_captions, captions.cpu().numpy()):\n",
    "            pred_caption = [ix_to_word[ix] for ix in pred if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_caption = [ix_to_word[ix] for ix in actual if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            bleu_score_value = sentence_bleu([actual_caption], pred_caption, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu_score_value)\n",
    "\n",
    "# Calculate average loss and BLEU score\n",
    "test_loss /= len(test_loader)\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Average BLEU Score: {average_bleu_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835757-d22e-4b12-87eb-8f3d1dd1d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8a15cd-04d8-43a8-ac68-f3333232c924",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "649aca55-b128-4921-9358-963cb6053d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "import gc\n",
    "import csv\n",
    "import pickle\n",
    "import timm\n",
    "import fractions\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.optim import Adam\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk.translate.bleu_score as bleu_score\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "data_dir = os.path.join('dataset')\n",
    "# data_dir = os.path.join('dataset30K')\n",
    "working_dir = os.path.join('working')\n",
    "images_dir = os.path.join(data_dir,'Images')\n",
    "captions_dir = os.path.join(data_dir,'captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21899a99-dfd8-4c91-a52f-63218bd17d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA GeForce GTX 1660 SUPER\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "# *********************\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc0978d-b76b-4fcc-9655-ba170a3ea9ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.030)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.030)\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.057)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.057)\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Identity()\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pretrained Swin Transformer model\n",
    "swin_model = timm.create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=0)  # num_classes=0 removes the classification head\n",
    "swin_model.to(device)\n",
    "swin_model.eval()\n",
    "\n",
    "# # Freeze the Swin model parameters\n",
    "# for param in swin_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67dd62e3-263d-453f-984e-fce459b644b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load a pretrained Swin Transformer model\n",
    "# swin_model = timm.create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=0)  # num_classes=0 removes the classification head\n",
    "# swin_model.to(device)\n",
    "# swin_model.train()  # Set to training mode\n",
    "\n",
    "# # Optionally, you might still want to freeze some layers if you don't want to train the entire model\n",
    "# # Example: Freezing earlier layers but training later layers\n",
    "# for name, param in swin_model.named_parameters():\n",
    "#     if 'stage4' in name:  # Just an example, adjust according to your needs\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fa705c-786a-477b-aaaa-66e441d2cc9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\anaconda3\\Lib\\site-packages\\transformers\\models\\detr\\feature_extraction_detr.py:38: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DetrForObjectDetection(\n",
       "  (model): DetrModel(\n",
       "    (backbone): DetrConvModel(\n",
       "      (conv_encoder): DetrConvEncoder(\n",
       "        (model): FeatureListNet(\n",
       "          (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (bn1): DetrFrozenBatchNorm2d()\n",
       "          (act1): ReLU(inplace=True)\n",
       "          (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (layer1): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer2): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer3): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (3): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (4): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (5): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (layer4): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): DetrFrozenBatchNorm2d()\n",
       "              )\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (2): Bottleneck(\n",
       "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): DetrFrozenBatchNorm2d()\n",
       "              (act1): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): DetrFrozenBatchNorm2d()\n",
       "              (drop_block): Identity()\n",
       "              (act2): ReLU(inplace=True)\n",
       "              (aa): Identity()\n",
       "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): DetrFrozenBatchNorm2d()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (position_embedding): DetrSinePositionEmbedding()\n",
       "    )\n",
       "    (input_projection): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (query_position_embeddings): Embedding(100, 256)\n",
       "    (encoder): DetrEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrEncoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): DetrDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x DetrDecoderLayer(\n",
       "          (self_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): DetrAttention(\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_labels_classifier): Linear(in_features=256, out_features=92, bias=True)\n",
       "  (bbox_predictor): DetrMLPPredictionHead(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained DETR model and feature extractor\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "detr_model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "detr_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cec4105-ec11-4b35-9d87-aa56b183682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPT tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # You can use 'gpt2-medium' or 'gpt2-large' for larger models\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# # Resize GPT's token embedding if you plan to add special tokens like <PAD> or <BOS>\n",
    "# tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a0c7b-2213-4352-9335-0605569a95fe",
   "metadata": {},
   "source": [
    "# Preprocess & encode captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eded2ad0-2035-4731-a85e-4bee171d92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for flickr8k\n",
    "def load_captions(filepath):\n",
    "    captions = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file) \n",
    "        for row in reader:\n",
    "            if len(row) != 2:\n",
    "                print(f\"Skipping malformed line: {row[:50]}...\")  \n",
    "                continue\n",
    "            image_id, caption = row\n",
    "            image_id = image_id.split('.')[0]  \n",
    "            if image_id not in captions:\n",
    "                captions[image_id] = []\n",
    "            captions[image_id].append(caption)\n",
    "    \n",
    "    return captions\n",
    "captions = load_captions(captions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6308aa5a-4983-4aa1-9420-e63bcdd22c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_caption(caption, tokenizer, max_length):\n",
    "    # Tokenize caption using GPT2 tokenizer\n",
    "    inputs = tokenizer(caption, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return inputs.input_ids, inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7ff5c4e-9e37-4e3a-8ab1-f26711510ba3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def clean_and_tokenize(caption):\n",
    "#     tokens = caption.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "#     return tokens\n",
    "    \n",
    "# # Collect all captions\n",
    "# all_captions = []\n",
    "# for cap_list in captions.values():\n",
    "#     all_captions.extend(cap_list)\n",
    "    \n",
    "# # Count word frequencies\n",
    "# word_counts = Counter()\n",
    "# for caption in all_captions:\n",
    "#     word_counts.update(clean_and_tokenize(caption))\n",
    "\n",
    "# # Create a vocabulary with words that exist in both Word2Vec and your dataset\n",
    "# vocab = [word for word, count in word_counts.items() if count >= 2]\n",
    "\n",
    "# # Map words to indices for the special tokens\n",
    "# word_to_ix = {word: ix for ix, word in enumerate(vocab, start=4)}  # start=4 to leave 0 for <PAD>, 1 for <START>, 2 for <END>, 3 for <UNK>\n",
    "# word_to_ix['<PAD>'] = 0\n",
    "# word_to_ix['<START>'] = 1\n",
    "# word_to_ix['<END>'] = 2\n",
    "# word_to_ix['<UNK>'] = 3\n",
    "\n",
    "# # Reverse lookup for decoding\n",
    "# ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "# # Update vocab size\n",
    "# vocab_size = len(word_to_ix)\n",
    "# vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58251b2-9575-4997-81f4-f1e15ebeec61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vocab_file_path = 'vocab.json'\n",
    "# with open('vocab.json', 'w') as vocab_file:\n",
    "#     json.dump(word_to_ix, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e039743-790c-4625-bcde-61a78447b1ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def encode_caption(caption, word_to_ix, max_length):\n",
    "#     tokens = clean_and_tokenize(caption)\n",
    "#     tokens = ['<START>'] + tokens + ['<END>']\n",
    "#     caption_ids = [word_to_ix.get(token, word_to_ix['<UNK>']) for token in tokens]\n",
    "#     if len(caption_ids) < max_length:\n",
    "#         caption_ids += [word_to_ix['<PAD>']] * (max_length - len(caption_ids))\n",
    "#     else:\n",
    "#         caption_ids = caption_ids[:max_length]\n",
    "#     return np.array(caption_ids)\n",
    "\n",
    "# max_length = max(len(clean_and_tokenize(caption)) + 2 for caption in all_captions)  # +2 for <START> and <END>\n",
    "# encoded_captions = {img_id: [encode_caption(caption, word_to_ix, max_length) for caption in cap_list]\n",
    "#                     for img_id, cap_list in captions.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f902c-8c13-49fd-808b-f4cf90b06f0e",
   "metadata": {},
   "source": [
    "# Features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee737071-ecab-4a14-91b5-0676d65ac504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_swin_features(swin_model, image_path, transform):\n",
    "    # Preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\") \n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = swin_model(image_tensor)\n",
    "    \n",
    "    # Swin transformer returns [batch_size, num_patches, embedding_dim]\n",
    "    # You can reshape or pool as needed for your LSTM input\n",
    "    return features.view(features.size(0), -1)  # Flatten into [batch_size, feature_dim]\n",
    "\n",
    "def extract_detr_features(detr_model, image_path, transform, target_size=1536):\n",
    "    # Load image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((224, 224))  # Adjust the size as needed\n",
    "    \n",
    "    # Apply the transformation (resize, normalize, etc.) to the image\n",
    "    inputs = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get outputs from the DETR model\n",
    "        outputs = detr_model(inputs)\n",
    "\n",
    "    # Extract object queries (decoder hidden states)\n",
    "    object_queries = outputs.last_hidden_state  # Output of the last decoder layer\n",
    "\n",
    "    # Flatten the object queries into a 2D tensor [batch_size, num_queries * hidden_dim]\n",
    "    flattened_object_queries = object_queries.view(object_queries.size(0), -1)\n",
    "\n",
    "    # If needed, reduce dimensionality via mean pooling to match target size (1536)\n",
    "    if flattened_object_queries.size(1) > target_size:\n",
    "        pooled_object_queries = torch.nn.functional.adaptive_avg_pool1d(\n",
    "            flattened_object_queries.unsqueeze(0), target_size).squeeze(0)\n",
    "    else:\n",
    "        pooled_object_queries = flattened_object_queries\n",
    "\n",
    "    return pooled_object_queries.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b461bbef-dab1-410f-8cde-5827cc458c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of image IDs (filenames without extension)\n",
    "image_ids = [img_name.split('.')[0] for img_name in os.listdir(images_dir) if img_name.endswith('.jpg')]\n",
    "\n",
    "# Set up transforms for training and validation/test\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Define the transform used during feature extraction (should be fixed)\n",
    "feature_extraction_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2404c378-5dd5-4908-82e5-f636df308a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using flickr8K dataset\n"
     ]
    }
   ],
   "source": [
    "# Load features from a .pkl file or extract them if not present\n",
    "if data_dir == 'dataset':\n",
    "    features_file = 'featuresMixed.pkl'\n",
    "    print('using flickr8K dataset')\n",
    "elif data_dir == 'dataset30K':\n",
    "    features_file = 'featuresMixed30K.pkl'\n",
    "    print('using flickr30K dataset')\n",
    "else:\n",
    "    raise ValueError(f\"Unrecognized dataset\")\n",
    "    \n",
    "features_dict = {}\n",
    "if os.path.exists(features_file):\n",
    "    with open(features_file, 'rb') as f:\n",
    "        features_dict = pickle.load(f)\n",
    "else:\n",
    "    # Extract features for each image and save them\n",
    "    for image_id in tqdm(image_ids):\n",
    "        image_path = os.path.join(images_dir, image_id + '.jpg')\n",
    "        if os.path.exists(image_path):\n",
    "            swin_features = extract_swin_features(swin_model,image_path, feature_extraction_transform)\n",
    "            detr_features = extract_detr_features(detr_model,image_path, feature_extraction_transform, target_size=1536)\n",
    "            features_dict[image_id] = (swin_features, detr_features)\n",
    "\n",
    "    with open(features_file, 'wb') as f:\n",
    "        pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048c3e5-8d16-4ffd-a417-6975bfb79984",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cc41097-1c3f-407f-801c-df8389e64ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, features_dict, captions, tokenizer, image_ids, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features_dict (dict): Dictionary mapping image IDs to feature tuples (swin_features, detr_features).\n",
    "            captions (list): List of captions for images.\n",
    "            image_ids (list): List of image IDs.\n",
    "            transform (callable, optional): Optional transform to be applied to the features.\n",
    "        \"\"\"\n",
    "        self.features_dict = features_dict \n",
    "        self.captions = captions\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        \n",
    "        # Retrieve swin_features and detr_features from the dictionary\n",
    "        swin_features, detr_features = self.features_dict[image_id]\n",
    "\n",
    "        # Apply clone and detach to each feature separately\n",
    "        swin_features = swin_features.clone().detach().float().to(device)\n",
    "        detr_features = detr_features.clone().detach().float().to(device)\n",
    "\n",
    "        # Flatten both features to 1D\n",
    "        swin_features_flat = swin_features.flatten()  # Shape: [1536]\n",
    "        detr_features_flat = detr_features.flatten()  # Shape: [25600]\n",
    "        \n",
    "        # Combine features\n",
    "        features = torch.cat([swin_features, detr_features], dim=1)\n",
    "        \n",
    "        # Retrieve all captions for this image\n",
    "        captions_for_image = self.captions[idx]\n",
    "        \n",
    "        # Tokenize all captions for this image\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        \n",
    "        for caption in captions_for_image:\n",
    "            encoding = tokenizer(caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=38)\n",
    "            input_ids_list.append(encoding['input_ids'].squeeze(0))  # Squeeze batch dimension\n",
    "            attention_mask_list.append(encoding['attention_mask'].squeeze(0))  # Squeeze batch dimension\n",
    "        \n",
    "        input_ids = torch.stack(input_ids_list)  # Stack all tokenized captions\n",
    "        attention_mask = torch.stack(attention_mask_list)  # Stack all attention masks\n",
    "        \n",
    "        return features, input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdc17753-380e-49e4-9573-fc4f2441c6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Filter out None values from the batch\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    \n",
    "    # If the batch is empty, return empty tensors\n",
    "    if len(batch) == 0:\n",
    "        return torch.tensor([]).to(device), torch.tensor([]).to(device), torch.tensor([]).to(device)\n",
    "    \n",
    "    # Unzip the batch into features, input_ids, and attention_mask\n",
    "    features, input_ids, attention_masks = zip(*batch)\n",
    "    \n",
    "    # Determine the maximum feature size in the batch\n",
    "    max_feature_size = max(feature.size(0) for feature in features)\n",
    "    \n",
    "    # Pad feature tensors to the maximum size\n",
    "    features = torch.stack([\n",
    "        torch.cat((feature.to(device), torch.zeros((max_feature_size - feature.size(0),), device=device)), dim=0)\n",
    "        for feature in features\n",
    "    ])\n",
    "    \n",
    "    # Determine the maximum caption length in the batch for input_ids and attention_masks\n",
    "    max_caption_length = max(ids.size(0) for ids in input_ids)\n",
    "    \n",
    "    # Pad input_ids to the maximum length\n",
    "    input_ids_padded = torch.stack([\n",
    "        torch.cat((ids.to(device), torch.zeros((max_caption_length - ids.size(0),), dtype=torch.long, device=device)), dim=0)\n",
    "        for ids in input_ids\n",
    "    ])\n",
    "    \n",
    "    # Pad attention_mask to the maximum length\n",
    "    attention_mask_padded = torch.stack([\n",
    "        torch.cat((mask.to(device), torch.zeros((max_caption_length - mask.size(0),), dtype=torch.long, device=device)), dim=0)\n",
    "        for mask in attention_masks\n",
    "    ])\n",
    "    \n",
    "    return features, input_ids_padded, attention_mask_padded\n",
    "    \n",
    "print(type(captions))  # This should print <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0d74df9-9b96-41f5-aacd-4e51f4f4d12c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle and split data\n",
    "image_ids = list(captions.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "\n",
    "# Calculate indices for splits\n",
    "total_images = len(image_ids)\n",
    "train_end = int(0.7 * total_images)\n",
    "val_end = int(0.9 * total_images)\n",
    "\n",
    "train_ids = image_ids[:train_end]\n",
    "val_ids = image_ids[train_end:val_end]\n",
    "test_ids = image_ids[val_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b59ec87e-45e2-4dcf-ad9e-a7c032348497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Flickr8kDataset(features_dict, captions, tokenizer, train_ids, train_transform)\n",
    "val_dataset = Flickr8kDataset(features_dict, captions, tokenizer, val_ids, test_transform)\n",
    "test_dataset = Flickr8kDataset(features_dict, captions, tokenizer, test_ids, test_transform)\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "batch_size= 16\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcfc9f81-3c94-4d4a-8e65-e12718359db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swin Features Size: torch.Size([1, 1536])\n",
      "DETR Features Size: torch.Size([1, 1536])\n",
      "Combined Feature Size: 3072\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"dataset/Images/133905560_9d012b47f3.jpg\"  # Replace with a valid image path\n",
    "swin_features = extract_swin_features(swin_model, sample_image_path, feature_extraction_transform).to(device)\n",
    "detr_features = extract_detr_features(detr_model, sample_image_path, feature_extraction_transform, target_size=1536).to(device)\n",
    "\n",
    "# Combine features (Swin + DETR) - you can concatenate or average them\n",
    "combined_features  = torch.cat([swin_features.flatten(), detr_features.flatten()], dim=0)\n",
    "\n",
    "# Get the feature size as an integer\n",
    "feature_size = combined_features.size(0)\n",
    "\n",
    "# Print feature size\n",
    "print(\"Swin Features Size:\", swin_features.size())\n",
    "print(\"DETR Features Size:\", detr_features.size())\n",
    "print(\"Combined Feature Size:\", feature_size)\n",
    "# feature_size = sample_features.size(1)\n",
    "# print(\"Feature Size:\", feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b09c47d-2813-4777-aa7b-58e0cde46e89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m     42\u001b[0m dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m---> 43\u001b[0m captioning_model \u001b[38;5;241m=\u001b[39m ImageCaptioningModel(feature_size, hidden_size, vocab_size, embed_size, dropout)\n\u001b[0;32m     44\u001b[0m captioning_model \u001b[38;5;241m=\u001b[39m captioning_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, feature_size, gpt_model, embed_size, dropout):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.gpt_model = gpt_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_size = self.gpt_model.config.n_embd\n",
    "        self.feature_fc = nn.Linear(feature_size, embed_size)  # Adapting Swin output to LSTM\n",
    "    \n",
    "    def forward(self, features, captions, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: Image features extracted (e.g., from Swin and DETR), size: [batch_size, feature_size]\n",
    "            input_ids: Tokenized captions (GPT-2 token IDs), size: [batch_size, seq_length]\n",
    "            attention_mask: Attention mask for GPT-2, size: [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Project features to embedding size\n",
    "        batch_size = features.size(0)\n",
    "        features = self.feature_fc(features)  # Should be of shape [batch_size, embed_size]\n",
    "        features = features.unsqueeze(1)# Add a sequence dimension: [batch_size, 1, embed_size]\n",
    "\n",
    "        # Project features to GPT-2's embedding size\n",
    "        features = self.feature_fc(features)  # Shape: [batch_size, gpt2_embedding_size]\n",
    "        features = features.unsqueeze(1)  # Add a sequence dimension: [batch_size, 1, gpt2_embedding_size]\n",
    "\n",
    "        # Concatenate image features with the input_ids (caption tokens)\n",
    "        # The feature vector is treated as the initial input for GPT-2, followed by the tokenized caption\n",
    "        embeddings = self.gpt2.transformer.wte(input_ids)  # Get word embeddings for input_ids\n",
    "        inputs_embeds = torch.cat((features, embeddings), dim=1)  # Concatenate along sequence dimension\n",
    "\n",
    "        # Pass through GPT-2\n",
    "        outputs = self.gpt2(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "\n",
    "        return outputs.logits  # Return logits for next-token prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_size = 1024\n",
    "dropout = 0.3\n",
    "captioning_model = ImageCaptioningModel(feature_size, hidden_size, vocab_size, embed_size, dropout)\n",
    "captioning_model = captioning_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68906ec-289a-4b6d-84c5-7586e9d4c723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d7c9f-2bdf-414f-8276-e4e5cfcb0aad",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c73c28-deb9-4d00-9e85-025ed63d522c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_ix['<PAD>'])\n",
    "optimizer = Adam(captioning_model.parameters(), lr=0.004, weight_decay=0.0001)\n",
    "# Add a StepLR scheduler - decays the learning rate by gamma every 'step_size' epochs\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.01, min_lr=1e-6)\n",
    "num_epochs = 60\n",
    "early_stopping = EarlyStopping(patience=4, delta=0.01)  # Adjust patience and delta as needed\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# Create a tqdm instance to show global progress for training and validation\n",
    "total_steps = num_epochs * len(train_loader) + num_epochs * len(val_loader)\n",
    "with tqdm(total=total_steps, desc='Training Progress') as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        captioning_model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i, (features, captions) in enumerate(train_loader):\n",
    "            if features.shape[0] == 0:  # Skip if batch is empty\n",
    "                continue\n",
    "                \n",
    "            # Move inputs and targets to the selected device (GPU or CPU)\n",
    "            features = features.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the size of the captions\n",
    "            batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "            # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "            features = features.squeeze(1).squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "\n",
    "            # Now expand the features to repeat them across the captions dimension\n",
    "            features = features.unsqueeze(1).expand(-1, num_captions, -1)  # [batch_size, num_captions, feature_size]\n",
    "\n",
    "            # Flatten features and captions for input to the model\n",
    "            features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "            captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = captioning_model(features, captions)\n",
    "            loss = criterion(outputs[:, :captions.size(1), :].view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            pbar.set_postfix({'Step': i, 'Loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_loss_values.append(epoch_train_loss)\n",
    "       \n",
    "        # Validation loop\n",
    "        captioning_model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, captions in val_loader:\n",
    "                if features.shape[0] == 0:  # Skip empty batches\n",
    "                    continue\n",
    "                # Move inputs and targets to the selected device (GPU or CPU)\n",
    "                features = features.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Get the size of the captions\n",
    "                batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "                # Squeeze and expand features just like in the training loop\n",
    "                features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "                features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "                features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "                captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = captioning_model(features, captions)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "        epoch_val_loss /= len(val_loader)\n",
    "        val_loss_values.append(epoch_val_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}],  ' f'Training Loss: {epoch_train_loss:.3f},  ' f'Validation Loss: {epoch_val_loss:.3f}')\n",
    "        \n",
    "        # Update the learning rate scheduler based on validation loss\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(epoch_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411abb0-1b3e-4a58-b438-101e9679ff7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save_path = 'captioning_model.pth'\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(captioning_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4fac2-d20d-4d35-8c5a-db734c89339e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path where the model is saved\n",
    "model_save_path = 'captioning_model.pth'\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "captioning_model.load_state_dict(torch.load(model_save_path))\n",
    "# Set the model to evaluation mode\n",
    "captioning_model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45dd76-776c-427c-99ad-337740e89c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save loss values to a file\n",
    "with open('train_loss_values.json', 'w') as f:\n",
    "    json.dump(train_loss_values, f)\n",
    "# Save loss values to a file\n",
    "with open('val_loss_values.json', 'w') as f:\n",
    "    json.dump(val_loss_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75875a4d-a458-4c74-8a4b-7e30617fe81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if not data:\n",
    "            print(f\"Warning: {filename} is empty.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {filename}.\")\n",
    "        return None\n",
    "\n",
    "# Load the loss values\n",
    "train_loss_values = load_json('train_loss_values.json')\n",
    "val_loss_values = load_json('val_loss_values.json')\n",
    "\n",
    "# show thier values\n",
    "train_loss_values ,val_loss_values\n",
    "\n",
    "# Simplified plot without specifying figsize\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.plot(val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81136913-e128-4feb-9683-25e339ad6f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Testing phase\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for features, captions in test_loader:\n",
    "        if features.shape[0] == 0:  # Skip empty batches\n",
    "            continue\n",
    "        # Move inputs and targets to the selected device (GPU or CPU)\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "                \n",
    "        # Get the size of the captions\n",
    "        batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "        # Squeeze and expand features like in training and validation loops\n",
    "        features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "        features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "        captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Compute average test loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a2d2b-40b3-4b04-9d12-4d2e4ce742ea",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652b12d-a02b-4ec8-92b9-886058a41389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = feature_extraction_transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "    \n",
    "def extract_swin_image_features(model, image_path):\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = swin_model(image_tensor)\n",
    "    return features.view(features.size(0), -1)\n",
    "\n",
    "def extract_detr_image_features(model, image_path, target_size=1536):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize((224, 224))\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
    "    pixel_values = inputs['pixel_values']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "    object_queries = outputs.last_hidden_state \n",
    "    \n",
    "    flattened_object_queries = object_queries.view(object_queries.size(0), -1) \n",
    "\n",
    "    # If needed, reduce dimensionality via mean pooling to match target size (1536)\n",
    "    if flattened_object_queries.size(1) > target_size:\n",
    "        pooled_object_queries = torch.nn.functional.adaptive_avg_pool1d(\n",
    "            flattened_object_queries.unsqueeze(0), target_size).squeeze(0)\n",
    "    else:\n",
    "        pooled_object_queries = flattened_object_queries\n",
    "\n",
    "    return pooled_object_queries.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb495eeb-0051-4e02-af54-40fdc9e99f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_repetitive_words(caption):\n",
    "    words = caption.split()\n",
    "    filtered_words = [words[0]]  # Initialize with the first word\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] != words[i - 1]:\n",
    "            filtered_words.append(words[i])\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def generate_caption(captioning_model, swin_model, detr_model, image_path, word_to_ix, ix_to_word, max_length, feature_size, beam_size=5):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using the trained captioning model and Swin Transformer for feature extraction.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Preprocess the image to extract features\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = feature_extraction_transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "         # Extract features from Swin Transformer\n",
    "        swin_features = extract_swin_image_features(swin_model, image_path).to(device)  # Swin features\n",
    "\n",
    "        # Extract features from DETR\n",
    "        detr_features = extract_detr_image_features(detr_model, image_path, target_size=1536).to(device)  # DETR features\n",
    "\n",
    "        # Combine features from both models\n",
    "        features = torch.cat([swin_features, detr_features], dim=1)  # Concatenate along the feature dimension\n",
    "\n",
    "    # Initialize beam search\n",
    "    beam = [([word_to_ix['<START>']], 0)]  # (sequence, score)\n",
    "    for _ in range(max_length):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            caption_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "            with torch.no_grad():\n",
    "                outputs = captioning_model(features, caption_tensor)\n",
    "            \n",
    "            # Get top beam_size predictions\n",
    "            top_k_scores, top_k_ids = torch.topk(outputs[0, -1], beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_seq = seq + [top_k_ids[i].item()]\n",
    "                new_score = score + top_k_scores[i].item()\n",
    "                new_beam.append((new_seq, new_score))\n",
    "\n",
    "        # Keep only the top beam_size sequences\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "        # Check for end token\n",
    "        if any(seq[-1] == word_to_ix['<END>'] for seq, _ in beam):\n",
    "            break\n",
    "\n",
    "    # Choose the best sequence\n",
    "    best_seq = max(beam, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Skip <START> and <END> tokens\n",
    "    caption = ' '.join([ix_to_word[ix] for ix in best_seq if ix not in [word_to_ix['<START>'], word_to_ix['<END>']]])\n",
    "\n",
    "    # Remove repetitive words from the caption\n",
    "    caption = remove_repetitive_words(caption)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f56967-a112-4ac6-8326-152d36b9753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to randomly select 5 images from the directory\n",
    "def get_random_images(image_dir, num_images=5):\n",
    "    all_images = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith(('jpg', 'jpeg', 'png'))]\n",
    "    return random.sample(all_images, num_images)\n",
    "\n",
    "# Function to generate caption and plot the image\n",
    "def generate_and_plot_image(image_path, captioning_model, swin_model, detr_model, word_to_ix, ix_to_word, max_length=30, feature_size=3072):\n",
    "    try:\n",
    "        # Synchronize CUDA (if available)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Load and process the image\n",
    "        imagez = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Generate caption using preloaded models\n",
    "        caption = generate_caption(captioning_model, swin_model, detr_model, image_path, word_to_ix, ix_to_word, max_length, feature_size, beam_size=3)\n",
    "        \n",
    "        # Convert image to numpy array\n",
    "        image_array = np.array(imagez)\n",
    "        \n",
    "        # Create a new figure\n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        # Plot the image and add the caption\n",
    "        ax.imshow(image_array)\n",
    "        ax.axis('off')  # Hide axes\n",
    "        ax.set_title(caption)\n",
    "        \n",
    "        # Save the image with caption\n",
    "        save_path = os.path.splitext(image_path)[0] + '_with_caption.png'\n",
    "        fig.savefig(save_path, bbox_inches='tight', pad_inches=0)  # Save the plot as a PNG file\n",
    "        \n",
    "        # Display the image and the caption\n",
    "        plt.show()  # Ensure the image is displayed with the caption\n",
    "        \n",
    "        # Clear the figure\n",
    "        plt.close(fig)  # Close the figure after displaying\n",
    "\n",
    "        # Free resources by clearing the GPU cache and forcing garbage collection\n",
    "        torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "        gc.collect()  # Force garbage collection to free RAM\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during caption generation for {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b59b2-7af6-42c9-aec2-cade4f6052eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate captions for 5 random images\n",
    "def caption_random_images(images_dir, captioning_model, swin_model, detr_model, word_to_ix, ix_to_word):\n",
    "    random_images = get_random_images(images_dir, num_images=5)\n",
    "    \n",
    "    for image_path in random_images:\n",
    "        print(f\"Generating caption for: {image_path}\")\n",
    "        generate_and_plot_image(image_path, captioning_model, swin_model, detr_model, word_to_ix, ix_to_word)\n",
    "\n",
    "# Call the function to generate captions for 5 random images\n",
    "caption_random_images(images_dir, captioning_model, swin_model, detr_model, word_to_ix, ix_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd91a25-be7d-4d5f-ba3b-031b0733fa4f",
   "metadata": {},
   "source": [
    "# Metrics scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe2271-4f38-448f-aff5-9a1ab666a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082748cf-ebad-44e9-9538-34c454aa44eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom fraction class to avoid the unexpected keyword argument '_normalize' issue\n",
    "class CustomFraction(fractions.Fraction):\n",
    "    def __new__(cls, numerator=0, denominator=None, _normalize=True):\n",
    "        return super().__new__(cls, numerator, denominator)\n",
    "\n",
    "# Override the modified_precision function in nltk\n",
    "bleu_score.Fraction = CustomFraction\n",
    "\n",
    "# Testing phase with BLEU score calculation and tqdm progress bar\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "bleu_scores = []\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "# Lists to collect actual and predicted captions for BLEU score calculation\n",
    "actual_captions = []\n",
    "predicted_captions_list = []\n",
    "\n",
    "# Use tqdm to track the progress of the testing phase\n",
    "with torch.no_grad():\n",
    "    for features, captions in tqdm(test_loader, desc=\"Testing\"):\n",
    "        if features.shape[0] == 0:\n",
    "            continue\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # Get the size of the captions\n",
    "        batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "        # Squeeze and expand features like in training and validation loops\n",
    "        features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "        features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "        captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Decode the outputs and collect captions\n",
    "        predicted_captions = outputs.argmax(2).cpu().numpy()\n",
    "        for pred, actual in zip(predicted_captions, captions.cpu().numpy()):\n",
    "            pred_caption = [ix_to_word[ix] for ix in pred if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_caption = [ix_to_word[ix] for ix in actual if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_captions.append(actual_caption)\n",
    "            predicted_captions_list.append(pred_caption)\n",
    "\n",
    "# Calculate BLEU scores for all captions\n",
    "bleu1 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n",
    "bleu2 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
    "bleu3 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n",
    "bleu4 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
    "\n",
    "# Calculate average loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'BLEU-1 Score: {bleu1}')\n",
    "print(f'BLEU-2 Score: {bleu2}')\n",
    "print(f'BLEU-3 Score: {bleu3}')\n",
    "print(f'BLEU-4 Score: {bleu4}')\n",
    "\n",
    "# Average bleu score\n",
    "bleu_scores = [bleu1, bleu2, bleu3, bleu4]\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f'Average BLEU Score: {average_bleu_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835757-d22e-4b12-87eb-8f3d1dd1d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

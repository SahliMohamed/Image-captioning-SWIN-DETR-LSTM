{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a395b46f-1095-4a07-adc9-73d49fe78991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import string\n",
    "import csv\n",
    "import pickle\n",
    "import timm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.optim import Adam\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision.models.detection.image_list import ImageList\n",
    "\n",
    "data_dir = os.path.join('dataset')\n",
    "working_dir = os.path.join('working')\n",
    "images_dir = os.path.join(data_dir,'Images')\n",
    "captions_dir = os.path.join(data_dir,'captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21899a99-dfd8-4c91-a52f-63218bd17d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0: NVIDIA GeForce GTX 1660 SUPER\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"No GPU devices available.\")\n",
    "# *********************\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b6e3d5-ec75-45a5-8a5a-d7f6970e6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "pretrained_embeddings_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(pretrained_embeddings_path, \n",
    "binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2dc589-c1f7-4ff5-94a2-f7008abb8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# # Example text data\n",
    "# documents = [\"This is the first sentence.\", \"Here is another one.\", \"And a third sentence.\"]\n",
    "\n",
    "# # Preprocess text\n",
    "# processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9316cbbe-44bf-4310-9985-ed082178a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eded2ad0-2035-4731-a85e-4bee171d92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for flickr8k\n",
    "def load_captions(filepath):\n",
    "    captions = {}\n",
    "    with open(filepath, 'r') as file:\n",
    "        reader = csv.reader(file) \n",
    "        for row in reader:\n",
    "            if len(row) != 2:\n",
    "                print(f\"Skipping malformed line: {row[:50]}...\")  \n",
    "                continue\n",
    "            image_id, caption = row\n",
    "            image_id = image_id.split('.')[0]  \n",
    "            if image_id not in captions:\n",
    "                captions[image_id] = []\n",
    "            captions[image_id].append(caption)\n",
    "    \n",
    "    return captions\n",
    "captions = load_captions(captions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787e6f8f-1059-476e-a1d2-f7430fa2dc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # for flickr30k\n",
    "# def load_captions(filepath):\n",
    "#     captions = {}\n",
    "#     # Adding encoding parameter to handle potential encoding issues\n",
    "#     with open(filepath, 'r', encoding='utf-8', errors='replace') as file:\n",
    "#         reader = csv.reader(file) \n",
    "#         for row in reader:\n",
    "#             if len(row) != 2:\n",
    "#                 print(f\"Skipping malformed line: {row[:50]}...\")  \n",
    "#                 continue\n",
    "#             image_id, caption = row\n",
    "#             image_id = image_id.split('.')[0]  \n",
    "#             if image_id not in captions:\n",
    "#                 captions[image_id] = []\n",
    "#             captions[image_id].append(caption)\n",
    "    \n",
    "#     return captions\n",
    "\n",
    "# captions = load_captions(captions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "925790e1-b03b-4ce3-aa5a-2593764a25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_and_tokenize(caption):\n",
    "#     tokens = simple_preprocess(caption)\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7ff5c4e-9e37-4e3a-8ab1-f26711510ba3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_and_tokenize(caption):\n",
    "    tokens = caption.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    return tokens\n",
    "    \n",
    "# Collect all captions\n",
    "all_captions = []\n",
    "for cap_list in captions.values():\n",
    "    all_captions.extend(cap_list)\n",
    "    \n",
    "# Count word frequencies\n",
    "word_counts = Counter()\n",
    "for caption in all_captions:\n",
    "    word_counts.update(clean_and_tokenize(caption))\n",
    "\n",
    "# Create a vocabulary with words that exist in both Word2Vec and your dataset\n",
    "vocab = [word for word, count in word_counts.items() if count >= 2]\n",
    "\n",
    "# Map words to indices for the special tokens\n",
    "word_to_ix = {word: ix for ix, word in enumerate(vocab, start=4)}  # start=4 to leave 0 for <PAD>, 1 for <START>, 2 for <END>, 3 for <UNK>\n",
    "word_to_ix['<PAD>'] = 0\n",
    "word_to_ix['<START>'] = 1\n",
    "word_to_ix['<END>'] = 2\n",
    "word_to_ix['<UNK>'] = 3\n",
    "\n",
    "# Reverse lookup for decoding\n",
    "ix_to_word = {ix: word for word, ix in word_to_ix.items()}\n",
    "\n",
    "# Update vocab size\n",
    "vocab_size = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d780643b-f355-4056-a32c-9420fa282d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5224"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c58251b2-9575-4997-81f4-f1e15ebeec61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_file_path = 'vocab.json'\n",
    "with open('vocab.json', 'w') as vocab_file:\n",
    "    json.dump(word_to_ix, vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd387c9-9fc3-40de-a44c-bf3e05f4bc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e039743-790c-4625-bcde-61a78447b1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_caption(caption, word_to_ix, max_length):\n",
    "    tokens = clean_and_tokenize(caption)\n",
    "    tokens = ['<START>'] + tokens + ['<END>']\n",
    "    caption_ids = [word_to_ix.get(token, word_to_ix['<UNK>']) for token in tokens]\n",
    "    if len(caption_ids) < max_length:\n",
    "        caption_ids += [word_to_ix['<PAD>']] * (max_length - len(caption_ids))\n",
    "    else:\n",
    "        caption_ids = caption_ids[:max_length]\n",
    "    return np.array(caption_ids)\n",
    "\n",
    "max_length = max(len(clean_and_tokenize(caption)) + 2 for caption in all_captions)  # +2 for <START> and <END>\n",
    "encoded_captions = {img_id: [encode_caption(caption, word_to_ix, max_length) for caption in cap_list]\n",
    "                    for img_id, cap_list in captions.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee737071-ecab-4a14-91b5-0676d65ac504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features(model, image_path, transform):\n",
    "    # Preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\") \n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor)\n",
    "    \n",
    "    # Swin transformer returns [batch_size, num_patches, embedding_dim]\n",
    "    # You can reshape or pool as needed for your LSTM input\n",
    "    return features.view(features.size(0), -1)  # Flatten into [batch_size, feature_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67dd62e3-263d-453f-984e-fce459b644b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load a pretrained Swin Transformer model\n",
    "# swin_model = timm.create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=0)  # num_classes=0 removes the classification head\n",
    "# swin_model.to(device)\n",
    "# swin_model.train()  # Set to training mode\n",
    "\n",
    "# # Optionally, you might still want to freeze some layers if you don't want to train the entire model\n",
    "# # Example: Freezing earlier layers but training later layers\n",
    "# for name, param in swin_model.named_parameters():\n",
    "#     if 'stage4' in name:  # Just an example, adjust according to your needs\n",
    "#         param.requires_grad = True\n",
    "#     else:\n",
    "#         param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cc0978d-b76b-4fcc-9655-ba170a3ea9ae",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerStage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.004)\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.004)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.009)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.013)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.017)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.017)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.022)\n",
       "        )\n",
       "        (2): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.026)\n",
       "        )\n",
       "        (3): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.030)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.030)\n",
       "        )\n",
       "        (4): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.035)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.035)\n",
       "        )\n",
       "        (5): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.039)\n",
       "        )\n",
       "        (6): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.043)\n",
       "        )\n",
       "        (7): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.048)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.048)\n",
       "        )\n",
       "        (8): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.052)\n",
       "        )\n",
       "        (9): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.057)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.057)\n",
       "        )\n",
       "        (10): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.061)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.061)\n",
       "        )\n",
       "        (11): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.065)\n",
       "        )\n",
       "        (12): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.070)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.070)\n",
       "        )\n",
       "        (13): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.074)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.074)\n",
       "        )\n",
       "        (14): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.078)\n",
       "        )\n",
       "        (15): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.083)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.083)\n",
       "        )\n",
       "        (16): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.087)\n",
       "        )\n",
       "        (17): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerStage(\n",
       "      (downsample): PatchMerging(\n",
       "        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.096)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.096)\n",
       "        )\n",
       "        (1): SwinTransformerBlock(\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): WindowAttention(\n",
       "            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (drop_path1): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Identity()\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pretrained Swin Transformer model\n",
    "swin_model = timm.create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=0)  # num_classes=0 removes the classification head\n",
    "swin_model.to(device)\n",
    "swin_model.eval()\n",
    "\n",
    "# # Freeze the Swin model parameters\n",
    "# for param in swin_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b461bbef-dab1-410f-8cde-5827cc458c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of image IDs (filenames without extension)\n",
    "image_ids = [img_name.split('.')[0] for img_name in os.listdir(images_dir) if img_name.endswith('.jpg')]\n",
    "\n",
    "# Set up transforms for training and validation/test\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "# Define the transform used during feature extraction (should be fixed)\n",
    "feature_extraction_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b40e43f-b7ab-4217-a73e-193c996f2f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Extract features for all images and save them in a dictionary\n",
    "# features_dict = {}\n",
    "# for image_id in tqdm(image_ids):\n",
    "#     image_path = os.path.join(images_dir, image_id + '.jpg')\n",
    "#     if os.path.exists(image_path):\n",
    "#         features = extract_features(swin_model,image_path,feature_extraction_transform)\n",
    "#         features_dict[image_id] = features  # Convert to list for JSON serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4bdd62c-cbfe-4cce-b31f-3236dae0387f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save features to a .pkl file\n",
    "# with open('features.pkl', 'wb') as f:  # 'wb' for write-binary\n",
    "#     pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d901c5c0-5c12-40dd-b3d9-54f36d645ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load features from a .pkl file\n",
    "with open('features.pkl', 'rb') as f:  # 'rb' for read-binary\n",
    "    features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82ef551a-52c2-40d2-90c4-cb6d31747af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Save 30k features to a .pkl file\n",
    "# with open('features30k.pkl', 'wb') as f:  # 'wb' for write-binary\n",
    "#     pickle.dump(features_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05940b97-d9d8-496c-83d9-18b2592d07c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load 30kfeatures from a .pkl file\n",
    "# with open('features30k.pkl', 'rb') as f:  # 'rb' for read-binary\n",
    "#     features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc41097-1c3f-407f-801c-df8389e64ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFlickr8kDataset\u001b[39;00m(Dataset):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features_dict , captions, encoded_captions, image_ids, transform):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_dict  \u001b[38;5;241m=\u001b[39m features_dict \n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, features_dict , captions, encoded_captions, image_ids, transform):\n",
    "        self.features_dict  = features_dict \n",
    "        self.captions = captions\n",
    "        self.encoded_captions = encoded_captions\n",
    "        self.image_ids = image_ids\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        \n",
    "        features = self.features_dict[image_id].clone().detach().float()\n",
    "        # Randomly choose one of the captions for this image\n",
    "        # captions_for_image = self.encoded_captions[image_id]\n",
    "        # caption_idx = np.random.randint(0, len(captions_for_image))  # Choose a random caption\n",
    "        \n",
    "        captions_for_image = np.array(self.encoded_captions[image_id])# caption = torch.tensor(captions_for_image[caption_idx], dtype=torch.long)\n",
    "        caption = torch.tensor(captions_for_image, dtype=torch.long)\n",
    "        \n",
    "        return features, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdc17753-380e-49e4-9573-fc4f2441c6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Filter out None values\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if len(batch) == 0:\n",
    "        return torch.tensor([]).to(device), torch.tensor([]).to(device)  # Return empty tensors if the batch is empty\n",
    "    \n",
    "    features, captions = zip(*batch)\n",
    "    \n",
    "    # Determine the maximum feature size\n",
    "    max_feature_size = max(feature.size(1) for feature in features)\n",
    "    \n",
    "    # Pad feature tensors to the maximum size\n",
    "    features = torch.stack([\n",
    "        torch.cat((feature.to(device), torch.zeros((feature.size(0), max_feature_size - feature.size(1)), device=device)), dim=1)\n",
    "        for feature in features\n",
    "    ])\n",
    "    \n",
    "    captions = torch.stack(captions).to(device)\n",
    "    return features, captions\n",
    "    \n",
    "print(type(captions))  # This should print <class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0d74df9-9b96-41f5-aacd-4e51f4f4d12c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random # Shuffle and split data\n",
    "image_ids = list(captions.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "\n",
    "# Calculate indices for splits\n",
    "total_images = len(image_ids)\n",
    "train_end = int(0.7 * total_images)\n",
    "val_end = int(0.9 * total_images)\n",
    "\n",
    "train_ids = image_ids[:train_end]\n",
    "val_ids = image_ids[train_end:val_end]\n",
    "test_ids = image_ids[val_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b59ec87e-45e2-4dcf-ad9e-a7c032348497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Flickr8kDataset(features_dict, captions, encoded_captions, train_ids, train_transform)\n",
    "val_dataset = Flickr8kDataset(features_dict, captions, encoded_captions, val_ids, test_transform)\n",
    "test_dataset = Flickr8kDataset(features_dict, captions, encoded_captions, test_ids, test_transform)\n",
    "\n",
    "# Create DataLoaders for each split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcfc9f81-3c94-4d4a-8e65-e12718359db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Size: 1536\n"
     ]
    }
   ],
   "source": [
    "sample_image_path = \"dataset/Images/133905560_9d012b47f3.jpg\"  # Replace with a valid image path\n",
    "sample_features = extract_features(swin_model, sample_image_path,feature_extraction_transform)\n",
    "feature_size = sample_features.size(1)\n",
    "print(\"Feature Size:\", feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b09c47d-2813-4777-aa7b-58e0cde46e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, vocab_size, embed_size, dropout):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)  # Dropout after LSTM\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.feature_fc = nn.Linear(feature_size, embed_size)  # Adapting Swin output to LSTM\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # Project features to embedding size\n",
    "        batch_size = features.size(0)\n",
    "        features = features.view(batch_size, -1)\n",
    "        features = self.feature_fc(features).unsqueeze(1)  # [batch_size, 1, embed_size]\n",
    "        \n",
    "        # Prepare LSTM inputs\n",
    "        embeddings = self.embedding(captions)\n",
    "        embeddings = self.dropout(embeddings)  # Dropout after embedding layer\n",
    "        inputs = torch.cat((features, embeddings[:, :-1, :]), dim=1)  # Concatenate features with captions\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        hiddens = self.lstm_dropout(hiddens)  # Dropout after LSTM hidden states\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hidden_size = 1024\n",
    "embed_size = 256\n",
    "dropout = 0.5  \n",
    "vocab_size\n",
    "captioning_model = ImageCaptioningModel(feature_size, hidden_size, vocab_size, embed_size, dropout)\n",
    "captioning_model = captioning_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c68906ec-289a-4b6d-84c5-7586e9d4c723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0.01):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2c73c28-deb9-4d00-9e85-025ed63d522c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/40]:   2%|▎         | 228/9120 [00:26<07:00, 21.12it/s, Step=0, Loss=3.45]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Validation Loss: 3.4184787086412016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/40]:   5%|▌         | 457/9120 [00:53<08:45, 16.47it/s, Step=0, Loss=3.2]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/40], Validation Loss: 3.0827246834250057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/40]:   8%|▊         | 685/9120 [01:21<08:29, 16.57it/s, Step=0, Loss=2.64]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/40], Validation Loss: 2.929031732035618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/40]:  10%|█         | 913/9120 [01:48<08:18, 16.47it/s, Step=0, Loss=2.77]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/40], Validation Loss: 2.838222625208836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/40]:  13%|█▎        | 1141/9120 [02:15<08:10, 16.28it/s, Step=0, Loss=2.77]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40], Validation Loss: 2.7835500100079704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/40]:  15%|█▌        | 1369/9120 [02:43<08:01, 16.10it/s, Step=0, Loss=2.64]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/40], Validation Loss: 2.7404459691515157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/40]:  18%|█▊        | 1597/9120 [03:11<07:57, 15.75it/s, Step=0, Loss=2.59]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/40], Validation Loss: 2.7155344299241606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/40]:  20%|██        | 1825/9120 [03:39<07:43, 15.75it/s, Step=0, Loss=2.63]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/40], Validation Loss: 2.694678348653457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/40]:  23%|██▎       | 2053/9120 [04:08<07:27, 15.79it/s, Step=0, Loss=2.42]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/40], Validation Loss: 2.669717784021415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/40]:  25%|██▌       | 2281/9120 [04:37<07:20, 15.52it/s, Step=0, Loss=2.31]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/40], Validation Loss: 2.661142157573326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/40]:  28%|██▊       | 2509/9120 [05:05<07:01, 15.68it/s, Step=0, Loss=2.2]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40], Validation Loss: 2.646244960672715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/40]:  30%|███       | 2737/9120 [05:34<06:49, 15.59it/s, Step=0, Loss=2.5]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/40], Validation Loss: 2.626915286569034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [14/40]:  33%|███▎      | 2965/9120 [06:03<06:29, 15.79it/s, Step=0, Loss=2.27]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/40], Validation Loss: 2.6193768463882745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [15/40]:  35%|███▌      | 3193/9120 [06:31<06:23, 15.46it/s, Step=0, Loss=2.33]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/40], Validation Loss: 2.6261795642329195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [16/40]:  38%|███▊      | 3421/9120 [07:00<06:12, 15.28it/s, Step=0, Loss=2.32]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/40], Validation Loss: 2.6153986641004976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [17/40]:  40%|████      | 3649/9120 [07:29<05:55, 15.39it/s, Step=0, Loss=2.43]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40], Validation Loss: 2.6060943229525697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [18/40]:  43%|████▎     | 3877/9120 [07:58<05:44, 15.23it/s, Step=0, Loss=2.2]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/40], Validation Loss: 2.61017323475258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [19/40]:  45%|████▌     | 4105/9120 [08:27<05:26, 15.35it/s, Step=0, Loss=2.25]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/40], Validation Loss: 2.6035492981181427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [20/40]:  48%|████▊     | 4333/9120 [08:56<05:11, 15.36it/s, Step=0, Loss=2.21]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/40], Validation Loss: 2.5968259689854643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [21/40]:  50%|█████     | 4561/9120 [09:26<05:03, 15.01it/s, Step=0, Loss=2.08]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/40], Validation Loss: 2.603459961274091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [22/40]:  53%|█████▎    | 4789/9120 [09:55<04:43, 15.29it/s, Step=0, Loss=2.25]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40], Validation Loss: 2.607053686590756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [22/40]:  55%|█████▌    | 5016/9120 [10:24<08:30,  8.03it/s, Validation Loss=2.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/40], Validation Loss: 2.5937503973642984\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_ix['<PAD>'])\n",
    "optimizer = Adam(captioning_model.parameters(), lr=0.0018, weight_decay=0.0001)\n",
    "num_epochs = 40\n",
    "early_stopping = EarlyStopping(patience=4, delta=0.01)  # Adjust patience and delta as needed\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "# Create a tqdm instance to show global progress for training and validation\n",
    "total_steps = num_epochs * len(train_loader) + num_epochs * len(val_loader)\n",
    "with tqdm(total=total_steps, desc='Training Progress') as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        captioning_model.train()\n",
    "        epoch_train_loss = 0\n",
    "        for i, (features, captions) in enumerate(train_loader):\n",
    "            if features.shape[0] == 0:  # Skip if batch is empty\n",
    "                continue\n",
    "                \n",
    "            # Move inputs and targets to the selected device (GPU or CPU)\n",
    "            features = features.to(device)\n",
    "            captions = captions.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the size of the captions\n",
    "            batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "            # Squeeze out the extra dimensions in `features` to make it [batch_size, feature_size]\n",
    "            features = features.squeeze(1).squeeze(1)  # Removing the unnecessary singleton dimensions\n",
    "\n",
    "            # Now expand the features to repeat them across the captions dimension\n",
    "            features = features.unsqueeze(1).expand(-1, num_captions, -1)  # [batch_size, num_captions, feature_size]\n",
    "\n",
    "            # Flatten features and captions for input to the model\n",
    "            features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "            captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = captioning_model(features, captions)\n",
    "            loss = criterion(outputs[:, :captions.size(1), :].view(-1, vocab_size), captions.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "            pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            pbar.set_postfix({'Step': i, 'Loss': loss.item()})\n",
    "            pbar.update(1)\n",
    "        \n",
    "        epoch_train_loss /= len(train_loader)\n",
    "        train_loss_values.append(epoch_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        captioning_model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, captions in val_loader:\n",
    "                if features.shape[0] == 0:  # Skip empty batches\n",
    "                    continue\n",
    "                # Move inputs and targets to the selected device (GPU or CPU)\n",
    "                features = features.to(device)\n",
    "                captions = captions.to(device)\n",
    "                \n",
    "                # Get the size of the captions\n",
    "                batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "                # Squeeze and expand features just like in the training loop\n",
    "                features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "                features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "                features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "                captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = captioning_model(features, captions)\n",
    "                loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "                epoch_val_loss += loss.item()\n",
    "                pbar.update(1)  # Update progress bar for validation batches\n",
    "\n",
    "        epoch_val_loss /= len(val_loader)\n",
    "        val_loss_values.append(epoch_val_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {epoch_val_loss}')\n",
    "\n",
    "        # Update tqdm progress bar to reflect the completion of an epoch\n",
    "        pbar.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        pbar.set_postfix({'Validation Loss': epoch_val_loss})\n",
    "\n",
    "        # Early stopping\n",
    "        early_stopping(epoch_val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7411abb0-1b3e-4a58-b438-101e9679ff7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to captioning_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'captioning_model.pth'\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(captioning_model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ed4fac2-d20d-4d35-8c5a-db734c89339e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from captioning_model.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mohamed\\AppData\\Local\\Temp\\ipykernel_11796\\3224590603.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  captioning_model.load_state_dict(torch.load(model_save_path))\n"
     ]
    }
   ],
   "source": [
    "# Define the path where the model is saved\n",
    "model_save_path = 'captioning_model.pth'\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "captioning_model.load_state_dict(torch.load(model_save_path))\n",
    "# Set the model to evaluation mode\n",
    "captioning_model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c45dd76-776c-427c-99ad-337740e89c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save loss values to a file\n",
    "with open('train_loss_values.json', 'w') as f:\n",
    "    json.dump(train_loss_values, f)\n",
    "# Save loss values to a file\n",
    "with open('val_loss_values.json', 'w') as f:\n",
    "    json.dump(val_loss_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75875a4d-a458-4c74-8a4b-7e30617fe81c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        if not data:\n",
    "            print(f\"Warning: {filename} is empty.\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {filename} not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from {filename}.\")\n",
    "        return None\n",
    "\n",
    "# Load the loss values\n",
    "train_loss_values = load_json('train_loss_values.json')\n",
    "val_loss_values = load_json('val_loss_values.json')\n",
    "\n",
    "# show thier values\n",
    "train_loss_values ,val_loss_values\n",
    "\n",
    "# Simplified plot without specifying figsize\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.plot(val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81136913-e128-4feb-9683-25e339ad6f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.620670887140127\n"
     ]
    }
   ],
   "source": [
    "# Testing phase\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for features, captions in test_loader:\n",
    "        if features.shape[0] == 0:  # Skip empty batches\n",
    "            continue\n",
    "        # Move inputs and targets to the selected device (GPU or CPU)\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "                \n",
    "        # Get the size of the captions\n",
    "        batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "        # Squeeze and expand features like in training and validation loops\n",
    "        features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "        features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "        captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "# Compute average test loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9652b12d-a02b-4ec8-92b9-886058a41389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = feature_extraction_transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "    \n",
    "def extract_image_features(image_path, model):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    image_tensor = preprocess_image(image_path).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(image_tensor).view(1, -1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb495eeb-0051-4e02-af54-40fdc9e99f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_repetitive_words(caption):\n",
    "    words = caption.split()\n",
    "    filtered_words = [words[0]]  # Initialize with the first word\n",
    "    \n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] != words[i - 1]:\n",
    "            filtered_words.append(words[i])\n",
    "    \n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def generate_caption(captioning_model, swin_model, image_path, word_to_ix, ix_to_word, max_length, feature_size, beam_size=5):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image using the trained captioning model and Swin Transformer for feature extraction.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Preprocess the image to extract features\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = feature_extraction_transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = extract_image_features(image_path, swin_model).to(device)  # Ensure features are on the same device\n",
    "        features = features.view(1, feature_size)\n",
    "\n",
    "    # Initialize beam search\n",
    "    beam = [([word_to_ix['<START>']], 0)]  # (sequence, score)\n",
    "    for _ in range(max_length):\n",
    "        new_beam = []\n",
    "        for seq, score in beam:\n",
    "            caption_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "            with torch.no_grad():\n",
    "                outputs = captioning_model(features, caption_tensor)\n",
    "            \n",
    "            # Get top beam_size predictions\n",
    "            top_k_scores, top_k_ids = torch.topk(outputs[0, -1], beam_size)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_seq = seq + [top_k_ids[i].item()]\n",
    "                new_score = score + top_k_scores[i].item()\n",
    "                new_beam.append((new_seq, new_score))\n",
    "\n",
    "        # Keep only the top beam_size sequences\n",
    "        beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "        \n",
    "        # Check for end token\n",
    "        if any(seq[-1] == word_to_ix['<END>'] for seq, _ in beam):\n",
    "            break\n",
    "\n",
    "    # Choose the best sequence\n",
    "    best_seq = max(beam, key=lambda x: x[1])[0]\n",
    "\n",
    "    # Skip <START> and <END> tokens\n",
    "    caption = ' '.join([ix_to_word[ix] for ix in best_seq if ix not in [word_to_ix['<START>'], word_to_ix['<END>']]])\n",
    "\n",
    "    # Remove repetitive words from the caption\n",
    "    caption = remove_repetitive_words(caption)\n",
    "    \n",
    "    return caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e966362a-c137-401c-9315-c44622a970b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during caption generation: name 'torch' is not defined\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "# Path to the image you want to caption\n",
    "image_path = 'dataset/Images/10815824_2997e03d76.jpg'\n",
    "# Generate caption\n",
    "feature_size = 1536\n",
    "max_length = 30  # Maximum caption length\n",
    "\n",
    "try:\n",
    "    # Ensure synchronization before running the caption generation\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Generate caption using preloaded models\n",
    "    caption = generate_caption(captioning_model, swin_model, image_path, word_to_ix, ix_to_word, max_length, feature_size, beam_size=3)\n",
    "    \n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Convert image to numpy array\n",
    "    image_array = np.array(image)\n",
    "    \n",
    "    # Create a new figure\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the image and add the caption\n",
    "    ax.imshow(image_array)\n",
    "    ax.axis('off')  # Hide axes\n",
    "    ax.set_title(caption)\n",
    "    \n",
    "    # Save the image with caption before showing it\n",
    "    fig.savefig('image_with_caption.png', bbox_inches='tight', pad_inches=0)  # Save the plot as a PNG file\n",
    "    \n",
    "    # Display the image and the caption\n",
    "    plt.show()  # Ensure the image is displayed with the caption\n",
    "    \n",
    "    # Free resources by clearing the figure\n",
    "    plt.close(fig)  # Close the current figure after saving the image\n",
    "    \n",
    "    # Manually clear GPU cache and force garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "    gc.collect()  # Force garbage collection to free RAM\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during caption generation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "082748cf-ebad-44e9-9538-34c454aa44eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:01<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.620670887140127\n",
      "BLEU-1 Score: 0.4200664208557216\n",
      "BLEU-2 Score: 0.2794557133311556\n",
      "BLEU-3 Score: 0.215315307599643\n",
      "BLEU-4 Score: 0.16172399759665515\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import fractions\n",
    "\n",
    "# Custom fraction class to avoid the unexpected keyword argument '_normalize' issue\n",
    "class CustomFraction(fractions.Fraction):\n",
    "    def __new__(cls, numerator=0, denominator=None, _normalize=True):\n",
    "        return super().__new__(cls, numerator, denominator)\n",
    "\n",
    "# Override the modified_precision function in nltk\n",
    "import nltk.translate.bleu_score as bleu_score\n",
    "bleu_score.Fraction = CustomFraction\n",
    "\n",
    "# Testing phase with BLEU score calculation and tqdm progress bar\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "bleu_scores = []\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "# Lists to collect actual and predicted captions for BLEU score calculation\n",
    "actual_captions = []\n",
    "predicted_captions_list = []\n",
    "\n",
    "# Use tqdm to track the progress of the testing phase\n",
    "with torch.no_grad():\n",
    "    for features, captions in tqdm(test_loader, desc=\"Testing\"):\n",
    "        if features.shape[0] == 0:\n",
    "            continue\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "        # Get the size of the captions\n",
    "        batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "        # Squeeze and expand features like in training and validation loops\n",
    "        features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "        features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "        captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Decode the outputs and collect captions\n",
    "        predicted_captions = outputs.argmax(2).cpu().numpy()\n",
    "        for pred, actual in zip(predicted_captions, captions.cpu().numpy()):\n",
    "            pred_caption = [ix_to_word[ix] for ix in pred if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_caption = [ix_to_word[ix] for ix in actual if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_captions.append(actual_caption)\n",
    "            predicted_captions_list.append(pred_caption)\n",
    "\n",
    "# Calculate BLEU scores for all captions\n",
    "bleu1 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(1, 0, 0, 0), smoothing_function=smoothing_function)\n",
    "bleu2 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing_function)\n",
    "bleu3 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing_function)\n",
    "bleu4 = corpus_bleu([[actual] for actual in actual_captions], predicted_captions_list, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing_function)\n",
    "\n",
    "# Calculate average loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'BLEU-1 Score: {bleu1}')\n",
    "print(f'BLEU-2 Score: {bleu2}')\n",
    "print(f'BLEU-3 Score: {bleu3}')\n",
    "print(f'BLEU-4 Score: {bleu4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e49e7c22-a0b7-4db4-944c-bc9f2aeafe9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 26/26 [00:02<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.620670887140127\n",
      "Average BLEU Score: 0.19049614634848383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import fractions\n",
    "\n",
    "# Custom fraction class to avoid the unexpected keyword argument '_normalize' issue\n",
    "class CustomFraction(fractions.Fraction):\n",
    "    def __new__(cls, numerator=0, denominator=None, _normalize=True):\n",
    "        return super().__new__(cls, numerator, denominator)\n",
    "\n",
    "# Override the modified_precision function in nltk\n",
    "import nltk.translate.bleu_score as bleu_score\n",
    "bleu_score.Fraction = CustomFraction\n",
    "\n",
    "# Testing phase with BLEU score calculation and tqdm progress bar\n",
    "captioning_model.eval()\n",
    "test_loss = 0\n",
    "bleu_scores = []\n",
    "smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "# Use tqdm to track the progress of the testing phase\n",
    "with torch.no_grad():\n",
    "    for features, captions in tqdm(test_loader, desc=\"Testing\"):\n",
    "        if features.shape[0] == 0:\n",
    "            continue\n",
    "        # Move inputs and targets to the selected device (GPU or CPU)\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "                \n",
    "        # Get the size of the captions\n",
    "        batch_size, num_captions, seq_len = captions.size()\n",
    "\n",
    "        # Squeeze and expand features like in training and validation loops\n",
    "        features = features.squeeze(1).squeeze(1)  # Remove unnecessary singleton dimensions\n",
    "        features = features.unsqueeze(1).expand(-1, num_captions, -1)  # Expand to match num_captions\n",
    "        features = features.contiguous().view(batch_size * num_captions, -1)\n",
    "\n",
    "        captions = captions.view(batch_size * num_captions, seq_len)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = captioning_model(features, captions)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Decode the outputs and calculate BLEU scores\n",
    "        predicted_captions = outputs.argmax(2).cpu().numpy()\n",
    "        for pred, actual in zip(predicted_captions, captions.cpu().numpy()):\n",
    "            pred_caption = [ix_to_word[ix] for ix in pred if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            actual_caption = [ix_to_word[ix] for ix in actual if ix not in {word_to_ix['<PAD>'], word_to_ix['<START>'], word_to_ix['<END>'], word_to_ix['<UNK>']}]\n",
    "            bleu_score_value = sentence_bleu([actual_caption], pred_caption, smoothing_function=smoothing_function)\n",
    "            bleu_scores.append(bleu_score_value)\n",
    "\n",
    "# Calculate average loss and BLEU score\n",
    "test_loss /= len(test_loader)\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Average BLEU Score: {average_bleu_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835757-d22e-4b12-87eb-8f3d1dd1d6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
